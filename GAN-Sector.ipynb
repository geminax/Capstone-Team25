{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python380jvsc74a57bd031e2364e1d696cab27f1c84da8c006b44abd7e4dbb1e594e2e73ff31da1a2a3d",
      "display_name": "Python 3.8.0 64-bit"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0-final"
    },
    "metadata": {
      "interpreter": {
        "hash": "31e2364e1d696cab27f1c84da8c006b44abd7e4dbb1e594e2e73ff31da1a2a3d"
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-_SLXyZ6PuE"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "import time\n",
        "import os\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import *\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from tensorflow.keras.optimizers import *\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.gridspec as gridspec\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "---\n",
        "### Checking GPU Availability\n",
        "---"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
      ]
    },
    {
      "source": [
        "---\n",
        "### Importing Data\n",
        "---"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXj8gKfXdsqP"
      },
      "source": [
        "X = np.load('./storage/1yeardata/X.npy')\n",
        "data = pd.read_pickle('./storage/1yeardata/data.pickle')\n",
        "meta = pd.read_pickle('./storage/1yeardata/meta.pickle')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcoJ5swDKHWC",
        "outputId": "e685bb1b-3150-44a5-f796-60787bd1fb1a"
      },
      "source": [
        "print(data.shape)\n",
        "print(meta.shape)\n",
        "print(X.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#lets get all gics sector names in a list (for later use)\n",
        "sector_tags = []\n",
        "for j in meta['GICS Sector']:\n",
        "    if j not in sector_tags:\n",
        "        sector_tags.append(j)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sector_tags"
      ]
    },
    {
      "source": [
        "---\n",
        "### Setting GICS Sector\n",
        "---"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "#getting all the tickers in a specific sector, subind likely wont be used until post-analysis\n",
        "#this begs the question, why sector training? Because it necessitates \"batch\" coding\n",
        "sector_ticks = []\n",
        "gics_sector = 'Consumer Staples'\n",
        "for j in meta.values:\n",
        "    if j[1] == gics_sector:\n",
        "        sector_ticks.append(j[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#visualizing sector stocks\n",
        "for j in sector_ticks:\n",
        "    print(meta[meta.Ticker == j])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Variables that need to be defined\n",
        "\n",
        "window_len = 5 #daily rows; val of 5 looks back on 5 days worth of trading data\n",
        "n_features = X.shape[2] #for later on / = 14\n",
        "num_times_train = X.shape[1] - 7*5 #how many time rows used for training **NOT ROLLING WINDOW BATCH NUMBER** that's less than this number by 'window_len'\n",
        "#this above number needed for scaling processes mostly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(len(sector_ticks))\n",
        "for j in sector_ticks:\n",
        "    plt.plot(data[j]['Close'])\n",
        "    plt.title(gics_sector +' Sector Close')\n",
        "    plt.grid('on')\n",
        "    plt.xlim(data.index[0],data.index[-1])"
      ]
    },
    {
      "source": [
        "---\n",
        "### Preprocessing\n",
        "---"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnubVyaWN59l"
      },
      "source": [
        "#X numpy structure:\n",
        "#X[Ticker # corresponded with meta index (499 of these)][datetime index # (X of these)][OHLC/Tis]\n",
        "X = X.astype('float32')\n",
        "\n",
        "#need to make a for loop that scales every ticker in sector_ticks and puts it into a dict with each ticker corresponding to arr of dimension (num_times_train, n_features)\n",
        "X_scaled = {}\n",
        "x_forecasteval = {}\n",
        "#where do i store scalers? in a dictionary!\n",
        "scalers = {}\n",
        "\n",
        "for j in sector_ticks:\n",
        "    index = meta.loc[meta['Ticker'] == j].index[0]\n",
        "    X_sectick = X[index]\n",
        "\n",
        "    #x_realpredicted is just for forecast evaluation purposes/those 5 days ahead that we are comparing our forecasts on, kind of like test data but not really\n",
        "    x_forecasteval[j] = X_sectick[-7*5:,0]\n",
        "\n",
        "    X_sectick = X_sectick[:-35]\n",
        "\n",
        "    scalers[j] = StandardScaler()\n",
        "    scalers[j] = scalers[j].fit(X_sectick)\n",
        "    X_sectick_scaled = scalers[j].transform(X_sectick)\n",
        "    #for when we have to invert predictions\n",
        "    #X_AAPL_inverted = scaler.inverse_transform(X_AAPL_scaled)\n",
        "\n",
        "    X_scaled[j] = X_sectick_scaled\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESoOb2kH8NSq"
      },
      "source": [
        "#now let's prepare our rolling windows, for an entire sector.. we work with another dict with 3 dimensional arrays in each ticker\n",
        "x_train = {}\n",
        "rollin_samps = num_times_train-window_len+1\n",
        "for j in X_scaled.keys():\n",
        "  x_train[j] = np.zeros((rollin_samps,window_len,n_features))\n",
        "  for i in range(rollin_samps):\n",
        "    x_train[j][i] = X_scaled[j][i:i+window_len,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#lets save our last datum in each ticker into own dict to save for forecast data later on\n",
        "x_forecastdata = {}\n",
        "for j in sector_ticks:\n",
        "    x_forecastdata[j] = x_train[j][-1]"
      ]
    },
    {
      "source": [
        "---\n",
        "### Network Functions\n",
        "---"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kr_GYXeM945_"
      },
      "source": [
        "def discriminator(in_shape,lr):\n",
        "  inputs = Input(shape=(in_shape))\n",
        "  d1 = Dense(64)(inputs)\n",
        "  LR1 = LeakyReLU()(d1)\n",
        "  d2 = Dense(16)(LR1)\n",
        "  LR2 = LeakyReLU()(d2)\n",
        "  outputs = Dense(1,activation='sigmoid')(LR2)\n",
        "  model_D = keras.Model(inputs=inputs, outputs=outputs, name='discriminator')\n",
        "\n",
        "  opt=Adam(lr=lr)\n",
        "  model_D.compile(loss='binary_crossentropy',optimizer=opt)\n",
        "  return model_D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWgrv9IBGQIt"
      },
      "source": [
        "def generator(in_shape,out_dim,neurons_lstm):\n",
        "  inputs = Input(in_shape,name='window_input')\n",
        "  LSTM1 = LSTM(neurons_lstm,input_shape=in_shape,return_sequences=False)(inputs)\n",
        "  DO1 = Dropout(0.5)(LSTM1)\n",
        "  #LSTM2 = LSTM(neurons_lstm,input_shape=in_shape,return_sequences=False)(DO1)\n",
        "  #DO2 = Dropout(0.5)(LSTM2)\n",
        "  D1 = Dense(out_dim)(DO1)\n",
        "  LR = LeakyReLU()(D1)\n",
        "  outputs = Reshape((1,out_dim))(LR)\n",
        "  model_G = keras.Model(inputs=inputs,outputs=outputs,name='generator')\n",
        "\n",
        "  return model_G"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9Hnt_bZqBJl"
      },
      "source": [
        "def gan_model(model_D, model_G,gen_input_shape, lr, lossweights):\n",
        "  model_D.trainable = False\n",
        "  ins = Input((gen_input_shape))\n",
        "  predicted = model_G(ins)\n",
        "  #concat with ins\n",
        "  predicted_concat = concatenate([ins,predicted],axis=1)\n",
        "  #print(predicted_concat.shape)\n",
        "  outs = model_D(predicted_concat)\n",
        "  model_GAN = keras.Model(inputs=ins,outputs=outs,name='GAN')\n",
        "\n",
        "  opt = Adam(lr=lr)\n",
        "  model_GAN.compile(loss=['binary_crossentropy','mean_squared_error'],loss_weights=lossweights ,optimizer=opt)\n",
        "  return model_GAN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "---\n",
        "### Training Functions\n",
        "---"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVy-PMmUXfIx"
      },
      "source": [
        "#not used in LSTM-GAN\n",
        "def noise_vector(latent_dim, batch_size):\n",
        "  noise = np.random.randn(latent_dim * batch_size)\n",
        "  noise = np.reshape(noise, (batch_size,1, latent_dim))\n",
        "  return noise"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzvurJLahmUC"
      },
      "source": [
        "def generated_sample(window_data, model):\n",
        "  #latent_space = noise_vector(latent_dim, batch_size)\n",
        "  window_data = window_data[np.newaxis,:,:]\n",
        "  x_predicted = model.predict(window_data)\n",
        "  x_fake = np.concatenate((window_data,x_predicted),axis=1)\n",
        "  y_fake = np.zeros((1,1))\n",
        "  return x_fake, y_fake"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McUgaNxnXlsr"
      },
      "source": [
        "def real_sample(window_data, lookahead_data):\n",
        "  lookahead_data = lookahead_data[-1,:]\n",
        "  lookahead_data = lookahead_data[np.newaxis,:]\n",
        "  x_real = np.concatenate((window_data,lookahead_data),axis=0)\n",
        "  x_real = x_real[np.newaxis,:,:]\n",
        "  y_real = np.ones((1,1))\n",
        "  return x_real, y_real"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMiNeFilbKZH"
      },
      "source": [
        "def train(model_D,model_G,model_GAN, dataset,ticker, epochs=1):\n",
        "  global generator_loss, discriminator_loss\n",
        "  generator_loss[ticker] = []\n",
        "  discriminator_loss[ticker] = []\n",
        "  for j in range(epochs):\n",
        "    for i in range(dataset.shape[0]-1): \n",
        "      #start with discriminator training:\n",
        "      X_real, y_real = real_sample(dataset[i],dataset[i+1])\n",
        "      X_fake, y_fake = generated_sample(dataset[i],model_G) \n",
        "      X, y = np.vstack((X_real,X_fake)), np.vstack((y_real,y_fake))\n",
        "      \n",
        "      disc_loss = model_D.train_on_batch(X,y)\n",
        "      \n",
        "      #adversarial training\n",
        "      X_GAN = dataset[i]\n",
        "      X_GAN = X_GAN[np.newaxis,:,:]\n",
        "      y_GAN = np.ones((1,1))\n",
        "      \n",
        "      GAN_loss = model_GAN.train_on_batch(X_GAN,y_GAN)\n",
        "      \n",
        "      generator_loss[ticker].append(GAN_loss)\n",
        "      discriminator_loss[ticker].append(disc_loss)\n",
        "\n",
        "      #if (i % 100) == 0:\n",
        "        #print('>%d:%d, disc_loss=%.4f, GAN_loss=%.4f' % (j+1,i,disc_loss,GAN_loss))\n",
        "      \n",
        "    #print('>%d,  disc_loss=%.4f , GAN_loss=%.4f' % (j+1,disc_loss,GAN_loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3riJmVDzxAS2"
      },
      "source": [
        "---\n",
        "### Training\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LzjJBYr3qgvd",
        "outputId": "ae4d1a77-7687-43bf-b60a-1195105b77a5",
        "tags": []
      },
      "source": [
        "disc_input_shape = (6,n_features)\n",
        "gen_input_shape = (window_len,n_features)\n",
        "loss_weights = [0.5,0.5]\n",
        "gan_learning_rate = 1e-3\n",
        "disc_learning_rate = 1e-3\n",
        "lstm_neurons = 16\n",
        "\n",
        "epochs=5\n",
        "\n",
        "generator_loss = {}\n",
        "discriminator_loss = {}\n",
        "\n",
        "disc_models = {}\n",
        "gen_models = {}\n",
        "GAN_models = {}\n",
        "\n",
        "now = dt.datetime.now()\n",
        "now = now.strftime('%Y-%m-%d_%H-%M-%S')\n",
        "i=1\n",
        "print('>>Starting ' + gics_sector + ' Training . . .')\n",
        "#wall time for entire sector\n",
        "start = time.time()\n",
        "for j in x_train.keys():\n",
        "    disc_models[j] = discriminator(disc_input_shape,disc_learning_rate)\n",
        "    gen_models[j] = generator(gen_input_shape,n_features,lstm_neurons)\n",
        "    GAN_models[j] = gan_model(disc_models[j],gen_models[j],gen_input_shape,gan_learning_rate,loss_weights)\n",
        "    \n",
        "    train(disc_models[j], gen_models[j], GAN_models[j], dataset=x_train[j] , ticker=j , epochs=epochs)\n",
        "    print('>>%d ticker(s) completed' % (i))\n",
        "    i += 1\n",
        "    \n",
        "end = str(dt.timedelta(seconds=(round(time.time() - start))))\n",
        "#playsound('narration.mp3')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "---\n",
        "### Visualizations\n",
        "---"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#take first key in x_train as part of example visualizations\n",
        "key = list(x_train.keys())[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#any one of models work for this part since its just architecture showcase\n",
        "plot_model(GAN_models[key],show_shapes=True,show_layer_names=True,to_file='storage/architectures/ganarchs/gan_arch_%s.png' % (now))\n",
        "plot_model(disc_models[key],show_shapes=True,show_layer_names=True,to_file='storage/architectures/discarchs/disc_arch_%s.png' % (now))\n",
        "plot_model(gen_models[key],show_shapes=True,show_layer_names=True,to_file='storage/architectures/genarchs/gen_arch_%s.png' % (now))\n",
        "img_disc = mpimg.imread('storage/architectures/discarchs/disc_arch_%s.png' % (now))\n",
        "img_gen = mpimg.imread('storage/architectures/genarchs/gen_arch_%s.png' % (now))\n",
        "img_gan = mpimg.imread('storage/architectures/ganarchs/gan_arch_%s.png' % (now))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#example visualization of acn TIs\n",
        "vszn_data = data['ACN']\n",
        "fig = plt.figure(figsize=(10,15))\n",
        "ax1 = fig.add_subplot(311)\n",
        "#fig.suptitle('ACN TIs')\n",
        "plt.plot(vszn_data['Close'])\n",
        "plt.plot(vszn_data['H_BAND'])\n",
        "plt.plot(vszn_data['L_BAND'])\n",
        "plt.plot(vszn_data['SMA25'])\n",
        "plt.plot(vszn_data['SMA50'])\n",
        "plt.plot(vszn_data['SMA100'])\n",
        "plt.plot(vszn_data['EMA25'])\n",
        "plt.plot(vszn_data['EMA50'])\n",
        "plt.plot(vszn_data['EMA100'])\n",
        "plt.legend(['close','hband','lband','sma25','sma50','sma100','ema25','ema50','ema100'])\n",
        "plt.grid('on')\n",
        "plt.xlim(vszn_data['Close'].index[0],vszn_data['Close'].index[-1])\n",
        "ax2 = fig.add_subplot(312)\n",
        "plt.plot(vszn_data['MACD'])\n",
        "plt.grid('on')\n",
        "plt.xlim(vszn_data['Close'].index[0],vszn_data['Close'].index[-1])\n",
        "plt.legend(['macdline'])\n",
        "ax3 = fig.add_subplot(313)\n",
        "plt.plot(vszn_data['RSI'])\n",
        "plt.legend(['rsi'])\n",
        "plt.grid('on')\n",
        "plt.xlim(vszn_data['Close'].index[0],vszn_data['Close'].index[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = 'Features Included: %d\\n\\nEpochs: %d\\nLSTM Neurons: %d\\nDiscriminator LR: %.4f\\nGAN LR: %.4f\\nGAN Loss Weights (BCE,MSE): [%.1f,%.1f]\\n\\nTraining Wall Time: %s' % (n_features,epochs,lstm_neurons,disc_learning_rate,gan_learning_rate,loss_weights[0],loss_weights[1],end)\n",
        "\n",
        "gs = gridspec.GridSpec(3,6)\n",
        "gs.update(wspace=0.45,hspace=.25)\n",
        "plt.figure(figsize=(10,15))\n",
        "ax1 = plt.subplot(gs[0,:2],)\n",
        "plt.title('Generator Architecture')\n",
        "plt.imshow(img_gen)\n",
        "plt.axis('off')\n",
        "ax2 = plt.subplot(gs[0,2:4],)\n",
        "plt.title('Discriminator Architecture')\n",
        "plt.imshow(img_disc)\n",
        "plt.axis('off')\n",
        "ax7 = plt.subplot(gs[0,4:],)\n",
        "plt.title('GAN Architecture')\n",
        "plt.imshow(img_gan)\n",
        "plt.axis('off')\n",
        "ax3 = plt.subplot(gs[2,:4],)\n",
        "plt.title('Training Set Predictions: ' + key +' Close 2020-2021')\n",
        "plt.plot(real_close)\n",
        "plt.plot(predictions_close)\n",
        "plt.legend(['Close','Predictions'])\n",
        "plt.xlabel('Trading Hours')\n",
        "ax4 = plt.subplot(gs[2,4:],)\n",
        "plt.axis('off')\n",
        "plt.text(0.0,0.35,s=text)\n",
        "ax5 = plt.subplot(gs[1,:3],)\n",
        "plt.plot(generator_loss[key])\n",
        "plt.xlabel('Inputs')\n",
        "plt.title('Generator Loss')\n",
        "ax6 = plt.subplot(gs[1,3:],)\n",
        "plt.plot(discriminator_loss[key])\n",
        "plt.xlabel('Inputs')\n",
        "plt.title('Discriminator Loss')\n",
        "fname = 'storage/combined_summaries/Summary_%s_.png' % (now)\n",
        "plt.savefig(fname,facecolor='white',bbox_inches = 'tight')"
      ]
    },
    {
      "source": [
        "---\n",
        "### Directory Creations\n",
        "---"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#because of below issue, need to do a one-time directory creation loop for IT sector, something like /storage/models/sectors/IT/(all tickers)/(disc/gen/gan)/final location\n",
        "#do this one at a time, so first, create all the folders in /sectors/ then do all the tickers in /Information Technology/ then (disc/gen/gan) in each ticker\n",
        "#using os.mkdir\n",
        "\n",
        "'''\n",
        "for j in sector_tags:\n",
        "    path = os.path.join('./storage/sector_resources',j)\n",
        "    os.mkdir(path=path)\n",
        "\n",
        "'''\n",
        "#now create all tick dirs in /sectors/gics_sector using sector_ticks\n",
        "\n",
        "for j in sector_ticks:\n",
        "    path = './storage/models/sectors/'+ gics_sector + '/' + j\n",
        "    os.mkdir(path=path)\n",
        "\n",
        "#now create disc/gen/gan dirs in each ticker\n",
        "\n",
        "for j in sector_ticks:\n",
        "    pathdisc = './storage/models/sectors/' + gics_sector + '/' + j + '/disc'\n",
        "    pathgen = './storage/models/sectors/' + gics_sector + '/' + j + '/gen'\n",
        "    pathgan = './storage/models/sectors/' + gics_sector + '/' + j + '/gan'\n",
        "    os.mkdir(pathdisc)\n",
        "    os.mkdir(pathgen)\n",
        "    os.mkdir(pathgan)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PNgDdBTw9PU"
      },
      "source": [
        "---\n",
        "### Saving\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#saving forecastevals\n",
        "pickle_out = open('./storage/sector_resources/' + gics_sector + '/forecasteval.pickle','wb')\n",
        "pickle.dump(x_forecasteval,pickle_out)\n",
        "pickle_out.close()\n",
        "\n",
        "#saving forecastdata\n",
        "pickle_out = open('./storage/sector_resources/' + gics_sector + '/forecastdata.pickle','wb')\n",
        "pickle.dump(x_forecastdata,pickle_out)\n",
        "pickle_out.close()\n",
        "\n",
        "#saving scalers\n",
        "pickle_out = open('./storage/sector_resources/' + gics_sector + '/scaler.pickle','wb')\n",
        "pickle.dump(scalers,pickle_out)\n",
        "pickle_out.close()\n",
        "\n",
        "print('Sector Resources of ' + gics_sector + ' completed.')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UC_3JIS4l9WS",
        "outputId": "5fc3bb83-4c63-417f-b4e5-f9c3db9ea177"
      },
      "source": [
        "#savings models\n",
        "for j in sector_ticks:\n",
        "    disc_models[j].save('storage/models/sectors/' + gics_sector + '/' + j + '/disc/model_discriminator_%s' % (now),save_format=\"h5\")\n",
        "    gen_models[j].save('storage/models/sectors/' + gics_sector + '/' + j + '/gen/model_generator_%s' % (now),save_format=\"h5\")\n",
        "    GAN_models[j].save('storage/models/sectors/' + gics_sector + '/' + j + '/gan/model_GAN_%s' % (now),save_format=\"h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "---\n",
        "### Loading\n",
        "---"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#loading dict sector forecastevals\n",
        "pickle_in = open('./storage/sector_resources/' + gics_sector + '/forecasteval.pickle','rb')\n",
        "imptd_forecasteval = pickle.load(pickle_in)\n",
        "\n",
        "#loading dict sector forecastdata\n",
        "pickle_in = open('./storage/sector_resources/' + gics_sector + '/forecastdata.pickle','rb')\n",
        "imptd_forecastdata = pickle.load(pickle_in)\n",
        "\n",
        "#loading dict sector scalers\n",
        "pickle_in = open('./storage/sector_resources/' + gics_sector + '/scaler.pickle','rb')\n",
        "imptd_scalers = pickle.load(pickle_in)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEwiQ9tmv7QL",
        "outputId": "ba0dd8bf-d8a6-43be-b8e3-b80fa4bce8cb"
      },
      "source": [
        "#loading dict gen models\n",
        "ToT = '2021-04-14_09-59-34' # time string after model names/serves as a sort of 2FA for importing the correct models\n",
        "imptd_sector_generators = {}\n",
        "for j in sector_ticks:\n",
        "    imptd_sector_generators[j] = tf.keras.models.load_model('storage/models/sectors/' + gics_sector + '/' + j + '/gen/model_generator_%s' % (ToT),compile=False)\n",
        "    print('>>' + j + ' loaded')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "---\n",
        "### Forecasting Ahead\n",
        "---"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# a function that uses the same scaler used to transform the data, back to its original scaling (prices) by concatenating the data we want transformed\n",
        "# back to its original shape with a bunch of meaningless data (in this case, zeros)\n",
        "def inverse_and_supply(data,scaler):\n",
        "    inverted = np.concatenate((data,np.zeros((num_times_train-window_len+1,n_features))))\n",
        "    inverted = scaler.inverse_transform(inverted)\n",
        "    return inverted[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# a function that just tries to estimate better trend slopes by multiplying current trend by (its current close price / sum of all the close prices in that sub industry)\n",
        "# the format of this 'array' input: 1st column is trend/slope, 2nd column is last close price witnessed, 3rd column, is final weighted trend/slope\n",
        "def weighted_trend(array):\n",
        "    closing_sum = sum(array[:,1])\n",
        "    array[:,2] = (array[:,1] / closing_sum) * array[:,0]\n",
        "    weighted = sum(array[:,2])\n",
        "    return weighted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "#creates a copy of the imported forecastdata which is just the last sample in the training dataset not used\n",
        "rolling_forecastdata = dict(imptd_forecastdata)\n",
        "#new dict for predicted values\n",
        "predicted = {}\n",
        "for j in sector_ticks:\n",
        "    for i in range(window_len):\n",
        "        #create empty appendable array in each key within predicted\n",
        "        if j not in predicted.keys():\n",
        "            predicted[j] = np.empty((0,n_features))\n",
        "        #generate a sample by inputting the current sample in rolling forecastdata and the model necessary to predict on this ticker\n",
        "        temp,_ = generated_sample(rolling_forecastdata[j], imptd_sector_generators[j])\n",
        "\n",
        "        # append to the empty array with a row vector corresponding to the prediction. temp[0,5], because generators output size (1,6,n_features) and we only want the last row vector          # within this shape\n",
        "        predicted[j] = np.append(predicted[j],np.array([temp[0,5]]),axis=0)\n",
        "\n",
        "        # adding a dimension to this row vector to use for more post-processing\n",
        "        row_predicted_2d = np.array([predicted[j][i]])\n",
        "\n",
        "        # rolling the forecast data we want to predict on, one time-step forward, so essentially popping the first element and appending the predicted value\n",
        "        rolling_forecastdata[j] = np.concatenate((rolling_forecastdata[j][1:],row_predicted_2d))\n",
        "\n",
        "        # performing an inverse transform so that the data within predicted isnt scaled and in interpretable prices\n",
        "        predicted[j][i] = inverse_and_supply(row_predicted_2d,imptd_scalers[j])\n",
        "    print('>>' + j + ' Predicted')\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#new empty dict for trends / keys this time will be by sub-industry\n",
        "trends = {}\n",
        "for j in sector_ticks:\n",
        "\n",
        "    # finding out the sub-ind for each ticker\n",
        "    subind = meta[meta.Ticker == j]['GICS Sub-Industry'].iloc[0]\n",
        "\n",
        "    # adding it as a key and giving it an empty array if it does not exist\n",
        "    # this empty array within each sub-industry key allows itself to be appended with however many tickers\n",
        "    # correspond to this sub-industry\n",
        "    if subind not in trends.keys():\n",
        "        trends[subind] = np.empty((0,3)) #3 columns because 2 are used for closing and trend below, while third is calculated later within defined function\n",
        "\n",
        "    # last closing price in the ticker's prediction set which is always length : window_len (which is 5)  \n",
        "    closing = predicted[j][-1,0]\n",
        "\n",
        "    # trend calculation, first close price + last close price / timesteps in between\n",
        "    trend = (predicted[j][0,0] + closing) / window_len\n",
        "\n",
        "    # temp variable for storing these values within the trends dict\n",
        "    temp = np.array([[trend,closing,0]])\n",
        "\n",
        "    # finally appending these variables to trends\n",
        "    trends[subind] = np.append(trends[subind],temp,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# performing a weighted trend calculation that just replaces the entire array with a scalar\n",
        "for j in trends.keys():\n",
        "    trends[j] = weighted_trend(trends[j])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lists = sorted(trends.items(), key=lambda kv: kv[1], reverse=True)\n",
        "x, y = zip(*lists)\n",
        "\n",
        "plt.grid('on',zorder=0)\n",
        "plt.barh(x,y,color='orange',zorder=3)\n",
        "plt.title(gics_sector + ' Sub-Industry Trends')\n",
        "plt.xlabel('Predicted Closing Price Weighted Growth Rate')\n",
        "plt.savefig('./storage/trends/' + gics_sector + '.png',facecolor='white',bbox_inches = 'tight')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}